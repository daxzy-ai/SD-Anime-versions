{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bPyY7QA7Ud68"
      ],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daxzy-ai/SD-Anime-versions-webui/blob/main/N__FU_Web.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good anon, took the NAI files apart and put together a working web generator that works exactly like the original. \n",
        "\n",
        "I adapted it into a collab"
      ],
      "metadata": {
        "id": "CX9IagGrYBzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Colab was created by user: daswer123**\n",
        "\n",
        "https://github.com/daswer123/stable-diffusion-colab\n",
        "\n",
        "**If you liked the colab, give it a star :)**"
      ],
      "metadata": {
        "id": "iJrL4EKeAbKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SETUP"
      ],
      "metadata": {
        "id": "bPyY7QA7Ud68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n",
        "\n",
        "#Func to change model Path\n",
        "def replace_line(file_name, line_num, text):\n",
        "    lines = open(file_name, 'r').readlines()\n",
        "    lines[line_num] = text\n",
        "    out = open(file_name, 'w')\n",
        "    out.writelines(lines)\n",
        "    out.close()\n",
        "\n",
        "def replace_line_in_file(file_name, line_to_search, text_to_replace):\n",
        "    with open(file_name, 'r') as file:\n",
        "        # read a list of lines into data\n",
        "        data = file.readlines()\n",
        "\n",
        "    for line in data:\n",
        "        # if the line contains the string we're looking for,\n",
        "        # write the line to the output file\n",
        "        if line_to_search in line:\n",
        "            replace_line(file_name, data.index(line), text_to_replace)\n",
        "\n",
        "\n",
        "#UPDATE PYTHON TO 3.9.1\n",
        "!python --version\n",
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py39\" --user\n",
        "!python --version\n",
        "\n",
        "# !pip install pyngrok"
      ],
      "metadata": {
        "id": "xJDLXAkwLDB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install virtualenv\n",
        "!python --version\n",
        "!pip install uvicorn==0.18.3\n",
        "!npm install -g localtunnel\n",
        "!pip install gdown\n",
        "!gdown https://huggingface.co/Daswer123/gfdsa/resolve/main/stable-diffusion1_4.zip -O naifu.zip #pls don't like it\n",
        "!unzip naifu.zip\n",
        "!mv models /content/program/\n",
        "!rm -rf /content/naifu.zip #Remove unnecessary files"
      ],
      "metadata": {
        "id": "HfD0Lugl-9De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Versions\n",
        "replace_line(\"/content/program/requirements.txt\",1,\"torch==1.12.1\\n\")\n",
        "replace_line(\"/content/program/requirements.txt\",2,\"torchvision==0.13.1\\n\")\n",
        "replace_line(\"/content/program/requirements.txt\",3,\"torchaudio==0.12.1\\n\")\n",
        "\n",
        "replace_line(\"/content/program/requirements.txt\",12,\"pytorch-lightning==1.7.7\\n\")\n",
        "replace_line(\"/content/program/requirements.txt\",5,\"fastapi==0.85.0\\n\")\n",
        "replace_line(\"/content/program/requirements.txt\",6,\"uvicorn==0.18.3\\n\")\n",
        "replace_line(\"/content/program/requirements.txt\",7,\"omegaconf==2.2.3\\n\")\n",
        "replace_line(\"/content/program/requirements.txt\",8,\"transformers==4.23.1\\n\")\n",
        "replace_line(\"/content/program/requirements.txt\",16,\"jsonmerge==1.8.0\\n\")"
      ],
      "metadata": {
        "id": "oO6o9pR9FZsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/program\n",
        "!pip install -r /content/program/requirements.txt\n",
        "!wget https://pastebin.com/raw/fBYfa6hJ -O run.sh\n",
        "\n",
        "replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"models/animefull-final-pruned\"\\n')\n",
        "\n",
        "sfw_dowload = False\n",
        "custom_dowload = False\n",
        "anything_dowload = False\n",
        "anything_vae_dowload = False"
      ],
      "metadata": {
        "id": "nUoBhgaAC5Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instal xformers\n",
        "#Increases generation speed from 2.5it/s to 4it/s\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "from IPython.display import HTML\n",
        "from subprocess import getoutput\n",
        "import os\n",
        "s = getoutput('nvidia-smi')\n",
        "if 'T4' in s:\n",
        "  gpu = 'T4'\n",
        "elif 'P100' in s:\n",
        "  gpu = 'P100'\n",
        "elif 'V100' in s:\n",
        "  gpu = 'V100'\n",
        "elif 'A100' in s:\n",
        "  gpu = 'A100'\n",
        "\n",
        "if (gpu=='T4'):\n",
        "  %pip install -q https://github.com/daswer123/stable-diffusion-colab/raw/main/xformers%20prebuild/T4/python39/xformers-0.0.14.dev0-cp39-cp39-linux_x86_64.whl\n",
        "  \n",
        "elif (gpu=='P100'):\n",
        "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "\n",
        "elif (gpu=='V100'):\n",
        "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "\n",
        "elif (gpu=='A100'):\n",
        "  %pip install -q https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\n",
        "\n",
        "# #Need for xformers\n",
        "# %cd /content/program\n",
        "# !git clone https://github.com/openai/triton.git\n",
        "# %cd triton/python\n",
        "# !pip install -e ."
      ],
      "metadata": {
        "id": "f0yVgKl5jyBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select **full** or **sfw** model or you can upload your model\n",
        "\n",
        "## you can reselect without reloading\n",
        "The default setting is full\n",
        "\n",
        "Select custom only when you load your own model"
      ],
      "metadata": {
        "id": "JyDR5UEGpugB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Model = \"full\" #@param [\"full\",\"sfw\",\"anything\",\"custom\"]\n",
        "use_anything_vae = False #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown if you want to upload click the checkbox and write the path to your model, after launching your model will be transferred to the folder custom, after you have uploaded you must uncheck it, and now when you select custom your **model** will be loaded\n",
        "load_my_own_model = False #@param{type:\"boolean\"}\n",
        "#@markdown #####Write the folder or theass as path to the folder, you can leave the field blank if the model is in the root of the drive\n",
        "gdrive_path = \"SD_models/waifu-diffusion1_3.ckpt\" #@param {type:\"string\"}\n",
        "#@markdown Your path will look like /content/drive/MyDrive/**gdrive_path**\n",
        "load_model_via_hugginface = False #@param{type:\"boolean\"}\n",
        "hugginface_url = \"https://huggingface.co/hakurei/waifu-diffusion-v1-3/resolve/main/wd-v1-3-float32.ckpt\" #@param {type:\"string\"}\n",
        "#@markdown Insert a link like this\n",
        "\n",
        "#@markdown https://huggingface.co/hakurei/waifu-diffusion-v1-3/resolve/main/wd-v1-3-float32.ckpt\n",
        "\n",
        "if (load_my_own_model):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd /content/program/models\n",
        "    !mkdir custom\n",
        "    %cd custom\n",
        "\n",
        "    print(\"moving model to custom folder\")\n",
        "    new_gdrive_path = \"/content/drive/MyDrive/\" + gdrive_path\n",
        "    !cp $new_gdrive_path /content/program/models/custom/model.ckpt\n",
        "    !gdown https://huggingface.co/Daswer123/gfdsa/raw/main/sfw.yaml -O config.yaml\n",
        "    custom_dowload = True\n",
        "\n",
        "if (load_model_via_hugginface):\n",
        "  %cd /content/program/models\n",
        "  !mkdir custom\n",
        "  %cd custom\n",
        "\n",
        "  print(\"dowload model and moving it to custom folder\")\n",
        "  !gdown $hugginface_url -O model.ckpt\n",
        "  !gdown https://huggingface.co/Daswer123/gfdsa/raw/main/sfw.yaml -O config.yaml\n",
        "  custom_dowload = True\n",
        "\n",
        "if (Model == \"sfw\" and sfw_dowload == False):\n",
        "  %cd /content/program/models\n",
        "  !mkdir sfw\n",
        "  %cd sfw \n",
        "  !gdown https://huggingface.co/Daswer123/gfdsa/resolve/main/sfw.ckpt -O model.ckpt\n",
        "  !gdown https://huggingface.co/Daswer123/gfdsa/raw/main/sfw.yaml -O config.yaml\n",
        "  sfw_dowload = True;\n",
        "\n",
        "if (Model == \"anything\" and anything_dowload == False):\n",
        "  %cd /content/program/models\n",
        "  !mkdir anything\n",
        "  %cd anything \n",
        "  !gdown https://huggingface.co/Daswer123/misc_models/resolve/main/Anything-V3.0-pruned-fp32.ckpt -O model.ckpt\n",
        "  !gdown https://huggingface.co/Daswer123/gfdsa/raw/main/sfw.yaml -O config.yaml\n",
        "  anything_dowload = True;\n",
        "\n",
        "if (use_anything_vae == True and anything_vae_dowload == False):\n",
        "  %cd /content/program/models\n",
        "  !gdown https://huggingface.co/Daswer123/misc_models/resolve/main/Anything-V3.0.vae.pt -O /content/program/models/anything_vae.pt\n",
        "  anything_vae_dowload = True\n",
        "\n",
        "%cd /content/program\n",
        "\n",
        "if (use_anything_vae == True):\n",
        "  replace_line(\"run.sh\", 13, 'export VAE_PATH=\"models/anything_vae.pt\"\\n')\n",
        "else:\n",
        "  replace_line(\"run.sh\", 13, 'export VAE_PATH=\"models/animevae.pt\"\\n')\n",
        "\n",
        "if (Model == \"sfw\"):\n",
        "  replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"models/sfw\"\\n')\n",
        "  print(\"Your model is: SFW\")\n",
        "elif (Model == \"full\"):\n",
        "  replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"models/animefull-final-pruned\"\\n')\n",
        "  print(\"Your model is: FULL\")\n",
        "elif (Model == \"custom\" and custom_dowload):\n",
        "  replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"models/custom\"\\n')\n",
        "  print(\"Your model is: CUSTOM\")\n",
        "elif (Model == \"anything\"):\n",
        "  replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"models/anything\"\\n')\n",
        "  print(\"Your model is: ANYTHING\")\n",
        "else:\n",
        "  replace_line(\"run.sh\", 7, 'export MODEL_PATH=\"models/animefull-final-pruned\"\\n')\n",
        "  print(\"Your model must be loaded via gdrive before it can be run\")\n",
        "  print(\"Your model is: FULL\")"
      ],
      "metadata": {
        "id": "NBrmRspaJ8Er",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RUN \n",
        "Link to localtunel you can find in program folder in **lt.log**"
      ],
      "metadata": {
        "id": "PtUJD7OFppqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "%cd /content/program\n",
        "\n",
        "#@markdown If Localtunnel has stopped working or you want to use ngrok,\n",
        "use_ngrok = False #@param{type:\"boolean\"}\n",
        "#@markdown Get <b>your</b> token for ngrok [here](https://dashboard.ngrok.com/get-started/your-authtoken) \n",
        "ngrok_token = \"2HSKSXmR2Ip97Owdg3yIZLVdIk5_yayZbqeenwcgEuaxDP94\" #@param {type:\"string\"}\n",
        "\n",
        "if (use_ngrok):\n",
        "  # # Get your authtoken from https://dashboard.ngrok.com/auth\n",
        "  ngrok.set_auth_token(ngrok_token)\n",
        "  # # Terminate open tunnels if exist\n",
        "  ngrok.kill()\n",
        "  # ngrok.disconnect(6969)\n",
        "  time.sleep(3)\n",
        "  public_url = ngrok.connect(6969).public_url\n",
        "  \n",
        "%cd /content/program\n",
        "!nohup lt -l 0.0.0.0 -p 6969 > lt.log 2>&1 &  \n",
        "import time\n",
        "time.sleep(2)\n",
        "with open('/content/program/lt.log', 'r') as testwritefile:\n",
        "  print(\"\\033[92m\" + \"Wait until code loads then follow this link\")\n",
        "  print(testwritefile.read())\n",
        "  if (use_ngrok):\n",
        "    print(\"Your ngrok link:\")\n",
        "    print(public_url)\n",
        "  print(\"\\033[95m\")\n",
        "\n",
        "!bash run.sh"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yzSAl_7Hh0aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "1) Keep in mind that in NAI Full \"bad anatomy + low quality\" and \"low quality\" is the same as \"NSFW + bad anatomy + low quality\" and \"NSFW + low quality\" \n",
        "\n",
        "But NAI Sfw uses the same options as the original generator\n",
        "\n",
        "2) 99% of the time the result is identical to NAI Full and Sfw in their website\n",
        "\n",
        "3) Once you have loaded the model with hugginface or gdrive you can, uncheck the checkbox and just switch the model without loading again"
      ],
      "metadata": {
        "id": "wmSOGu5hoEbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RESTART COLAB, IF YOU GET SOME TROUBLE\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "IW5bEytDtxfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}